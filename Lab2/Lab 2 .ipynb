{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Ananya\\\\Downloads\\\\Datasets\\\\Spotifydata3.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the dimensions of the dataset\n",
    "size = df.size\n",
    "shape = df.shape\n",
    "\n",
    "print(\"Size = {}\\nShape ={}\\n\".format(size, shape, shape[0]*shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 10 rows of a dataset\n",
    "ten_rows = df.head(10)\n",
    "print(\"The first 10 rows of the dataset are as follows: \")\n",
    "print(ten_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the class (datatype) of each column of the dataset\n",
    "data_type = df.dtypes\n",
    "print(\"The classes of the columns are as follows: \")\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the number of null values in each column of the dataset\n",
    "null_values = df.isnull().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Calculate the percentage of null values in each column\n",
    "percentage = df.isna().sum()/len(df)*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Drop the columns with more than 50% of null values\n",
    "\"\"\"we first calculate the percentage of null values in each column using the isna and mean methods of the DataFrame. \n",
    "We then select the columns that have less than or equal to 50% null values using boolean indexing.\n",
    "Finally, we use the dropna method with the thresh parameter set to the number of non-null values required to keep a column. \n",
    "In this case, we set thresh to half the number of rows in the DataFrame, which means that any columns with less than half \n",
    "non-null values will be dropped.\n",
    "The resulting DataFrame, new_df, will contain only the columns that have less than or equal to 50% null values.\"\"\"\n",
    "null_per = df.isna().mean()*100\n",
    "selected_cols = null_per[null_per <= 50].index\n",
    "new_df = df[selected_cols].dropna(thresh=len(df)//2, axis=1)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Remove the duplicate observations from the dataset (row and column-wise) using drop_duplicate method\n",
    "# removing row-wise\n",
    "new = df.drop_duplicates()\n",
    "# removing column-wise\n",
    "new = df.T.drop_duplicates().T\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Remove the null values from the dataset by dropping the rows\n",
    "new = df.dropna()\n",
    "print(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Remove the null values from the dataset by dropping the columns\n",
    "new = df.dropna(axis=1)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Calculate the average of each numeric columns of the dataset\n",
    "avg = df.mean()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Display all the unique values of each column\n",
    "for col in df.columns:\n",
    "    unique_values = df[col].unique()\n",
    "    print(f\"Column {col} has {len(unique_values)} unique values: {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Display the count of unique values of each column\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"Column {col} has {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Fill the null values of each numeric columns with the average value (mean) without using the imputer method\n",
    "numeric = df.select_dtypes(include=np.number)\n",
    "numeric_columns = numeric.columns\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df.mean())\n",
    "print(df[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Fill the null values of categorical text columns with the most frequent value(mode) without using the imputer method\n",
    "# Identifying the categorical columns with the missing values in the df\n",
    "c_n = [col for col in df.columns if df[col].dtype == 'object' and df[col].isnull().any()] # For each categorical column with missing values, replace the null values with the most frequent value (mode) of that column\n",
    "for col in c_n:\n",
    "    mode_value = df[col].mode().values[0]\n",
    "    df[col].fillna(mode_value, inplace=True)\n",
    "\"\"\"This code first calculates the mode of each categorical column using the mode() method of Pandas, which returns a \n",
    "DataFrame of mode values for each column. We extract the first mode value from the DataFrame using .values[0] and \n",
    "replace the null values in the original column with the mode value using the fillna() method.\"\"\"\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Fill the null values of both categorical/text and numeric columns with the most frequent value (mode) by using sklearn library's imputer method.\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "print(df)\n",
    "\"\"\"A SimpleImputer instance is created with the strategy parameter set to 'most_frequent', which means the imputer will \n",
    "fill the null values with the most frequent value in the respective column.\n",
    "Next, the fit_transform method of the SimpleImputer instance is called on the DataFrame to fill the null values with the mode.\n",
    " Finally, the filled DataFrame is stored in the variable df, and the result is printed with the print function.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Fill the null values of both categorical/text and numeric columns with the mode by using sklearn library's imputer method on only specific rows and columns using iloc method.\n",
    "subset = df.iloc[2:10, 2:10]\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit(subset)  # computing the mode\n",
    "imputed_subset = imputer.transform(subset)  # performing scaling based on mode\n",
    "df.iloc[2:10, 2:10] = imputed_subset\n",
    "print(df.iloc[2:10, 2:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Use the describe() function to display the mean, standard deviation, and IQR values of the numeric columns of the dataset\n",
    "n_c = df.select_dtypes(include='number')\n",
    "n_s = n_c.describe().loc[['mean', 'std', '25%', '50%', '75%']]\n",
    "print(n_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Show the count of null values and missing values separately in each column of the dataset\n",
    "null_values = df.isnull().sum()\n",
    "missing_values = df.isna().sum()\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: \\t null={null_values[col]}, \\t missing={missing_values[col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Export the cleaned dataset to a new csv file without null or missing or duplicate value\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('cleaned_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
